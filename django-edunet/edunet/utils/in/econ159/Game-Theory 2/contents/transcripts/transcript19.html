<div id="fileContents"><h2><strong>Game Theory: </strong>Lecture 19 Transcript </h2><table cellspacing="0" cellpadding="0" id="transcriptHeader" summary="transcript header">	<tbody>		<tr>			<td id="transcriptDate">November 12, 2007</td>			<td id="transcriptNav"><span id="forwardNav"><a href="javascript:history.go(-1);">&lt;&lt; back</a></span></td>		</tr>	</tbody></table></p><p><b>Professor Ben Polak: </b>So last time we covered a whole bunch of new ideas, and it was really quite a lot of ideas for one class. Here's some of the ideas we covered. We talked about information sets, and these were ways to allow us to model imperfect information. So what's imperfect information? It's a way of being able to capture both simultaneity in moves and sequential moves in the same game. So it's a way that's going to allow us to meld the lessons from before the mid-term and after the mid-term. </p>
<p>Then we talked about what strategies meant in this context, and the basic idea is strategies are instructions--strategies for each player--give them an instruction at each of their information sets. Then we talked about what sub-games were, and, leaving aside technicalities, sub-games were just games within games. And finally we introduced the idea of sub-game perfection which is our new solution concept that refines the idea of Nash equilibrium. What sub-game perfection is going to do is it's going to instruct the players to play a Nash equilibrium in every sub-game. Another way of saying it is, a sub-game equilibrium is a Nash equilibrium in the whole game, but in each sub-game it induces Nash play as well. </p>
<p>Now, we're going to see today examples. If we have time I'll go through three different examples, and I'll tell you at the end of each example what it is I'm hoping to be able to take away from that example. So, last time was a lot of formal stuff. Today is going to be a lot of examples. Okay, that's our agenda. Here's a game. Here's our first example. And I call this example, I call this game, "don't screw up," for reasons we'll see in a minute. </p><p>So this is a game in which Player 1 has to choose between Up and Down. If Player 1 chooses Up then Player 2 gets to move and chooses between left and right. And if Player 2 chooses left then Player 1 gets to move again and Player 1 chooses between up or down. Everyone looking at that game? So why don't we play this game since we haven't played a game for a while. We'll play a couple of games today. </p><p>So what I'm going to do is let's divide the class in two. So if I just draw a line down the middle of the class, everybody to my left (to your right), everybody on this side of the class is a Player 1. Okay you're all Player 1's. And everyone on this side of class you're Player 2, including you guys hiding from the camera, you're Player 2's. Okay, so let's figure out what we're going to do. Everyone had the time to look at the game? So Player 1's you get to move first, those of you who are going to choose Down raise your hand now. Raise your hand. Wave it in the air. Keep it up so the camera can see you. And those of you who are going to choose Up raise your hands. </p><p>Lots more Ups. Those of you who chose Up why don't you all stand up. I don't want to do all the exercise here, so all those who chose Up, stand up. So you can see that choosing down ends the game, so this many people are still playing the game. Everyone who is still sitting down, everyone who sat down here has exited. All right, Player 2's you get to move now. So Player 2's, those of you who choose right, including the people on this aisle, those people who choose right raise your hand now--one right over there. Those of you who choose left raise your hands. Why don't you guys all stand up, just to get you awake on a Monday morning, everyone's sleepy otherwise. </p><p>Let's go back to Player 2's--sorry Player 1--those of you who are still in the game. So those of you who chose Up the first time, how many of you now choose down? Raise your hand if you choose down; and raise your hand if you choose up. Just to get a sample of this, let's get the 2's to sit down again so people can see them. So 2's sit down. Those of you Player 1's who are still in the game who were choosing up raise your hand now. I think that's everybody, is that correct? Okay, you can all sit down. So let's just talk about this game for a while and then we'll analyze it. </p><p>Now, this is not a difficult game from the point of view of stuff we've done since the mid-term. It's pretty clear what we should do in this game by backward induction. So why don't we start there. Okay, so by backward induction, we find that if Player 1 gets to move a second time then they're choosing between 4 and 3, and they're going to choose 4. Player 2, if they get to move, knowing that Player 1 is going to choose up tomorrow, they're going to be choosing between 3 if they choose left or 2 if they choose right. So they're going to stay in the game and choose left, which is what most of you did. Finally, Player 1 at the beginning of the game, knows that Player 2 is going to choose left whereupon she's going to choose up, so if she chooses Up she's going to end up getting 4, and if she chooses Down she's going to get 2, so she's going to choose Up. </p><p>So it's clear what backward induction does in this game and that's what most people did in the game. Is that right? However, not everybody did it. Some of the Player 1's actually, why don't you raise your hand, those people who chose Down--the ones who chose down. There were more than that. You can all stand up. Those of you who didn't stand up just now raise your hands. People are hiding now, but that's okay. Those people who chose Down, they may have had a reason for choosing Down, and their reason for choosing Down might have been that they thought--even though they can do backward induction--so even though they know that by backward induction Up gets them the better answer--they might be worried that if they choose Up, Player 2 will screw up and choose right. Notice that if Player 2 chooses right then Player 1 only gets 1, whereas Down yielded 2. So in some sense Down was the "safe" thing to do for Player 1 given that they might be worried that Player 2 might screw up. </p><p>Does that roughly--just nod if this is the case: for those people who chose down is that kind of what you were thinking? Some people are shaking their heads, but some people are nodding. That's a good sign. Now why might Player 2 in fact screw up and choose right. Because Player 2 might, themselves, think that Player 1 might screw up at this stage. If Player 1 were to screw up at the last stage and choose down, then Player 2 by choosing left would only get 1, and for him the safe option therefore is right which yields 2. So, to get the backward induction answer here--which most of us did--to get the backward induction answer here relies on Player 1 trusting Player 2 to play backward induction, and that requires Player 1 to have trust in Player 2 trusting Player 1 not to screw up in the last stage. So say it again, Player 1 needs Player 2 not to screw up, and that means Player 1 needs to trust that Player 2 will trust her not to screw up. </p><p>Everyone see the game? Okay, so let's try and analyze this game using what we learned last time and see what we find. So the first thing to do is let's look at strategies in this game. So Player 2 just has two strategies, left and right, because Player 2 only has one information set--and notice this game is actually a game of perfect information so it's going to be very easy. Player 1 has two information sets, this information set and that information set. At each of them Player 1 has two choices so she must have four strategies in all. So this game when we put it in its matrix form is going to be a 4 x 2 game. </p><p>Here it is. And the strategies for Player 1 are (Up, up), (Up, down), (Down, up), and (Down, down). And the strategies for Player 2 are just left and right. And now we can put the payoffs in. So ((Up, up), left) gets us (4, 3). ((Up, up), right) gets us (1, 2). ((Up, down), left) gets us (3, 1). ((Up, up, right) gets us (1, 2) again: we end up exiting the game here. (Down, up) is easy because it's just exiting the game at the first stage, so all of these are going to be just (2, 1). Everyone happy with that? So what I've done is translated the game into its matrix form. </p><p>And let's look for Nash equilibria in this game. Let me do it at the board since it's quite easy at this stage. So to look for Nash equilibria, let's just worry about pure strategy equilibria for now. So if Player 2 was choosing left then Player 1's best response is the (Up, up) strategy. And if Player 2 is choosing right, then Player 1's best response is either (Down, up) or (Down, down). That's exactly the conversation we just had. If Player 2 was going to "screw up" and choose right then Player 1 wants to get out of the game immediately. Conversely, if Player 1 is choosing (Up, up) then Player 2 is happy and is going to choose left, trusting Player 1. If Player 1 was going to choose (Up, down), however, that's Player 1 screwing up at the second stage. So in that case Player 2 wants to get out of the game and choose right. If Player 1 is choosing (Down, up) then Player 2 is actually--it doesn't matter they're indifferent. And if Player 2 is choosing (Down, down) then once again Player 2 is indifferent since they don't get to move at all. </p><p>So from this we see that there are three Nash equilibria, let me call them 1, 2, and 3. So one Nash equilibrium is ((Up, up), left). Another Nash equilibrium is ((Down, up, right): that's here. And the third equilibrium is ((Down, down), right). So there are three pure strategy Nash equilibria in this game. Let's just see what they do. So the first one ((Up, up), left) is Up, left, up so it gets us to here. So this one is the same equilibrium as corresponds to backward induction. Is that right? This one's the backward induction equilibrium, and the other two are different. ((Down, up), right) and ((Down, down), right) both end up down here exiting the game immediately, so both of these other equilibria fail backward induction. So let's put backward induction with a line through it: they're failing backward induction. </p><p>So you might ask why are they equilibria? We've seen examples like this before, for example, in the entry game. We looked at some examples last time. So, in this game, the reason these are equilibria even though they fail backward induction exactly coincides with that conversation we just had about worrying about the other person screwing up. So, in particular, if Player 1 thinks that Player 2 is choosing right--that is to say, thinks that Player 2 is going to screw up--then Player 1 doesn't want to travel up the tree because she knows she'll be carried down here, and instead she just chooses the safe option and gets 2. So from Player 1's point of view, if Player 2 was going to choose right, then getting out of the game doing the safe thing is the best response for Player 1. And for Player 2's point of view, if Player 1's exiting the game it really doesn't matter what Player 2 says she's going to do, because she doesn't get to move anyway so that's why these are both equilibria. </p><p>Okay, so what we're going to do next, we've translated this into a tree. We've written down the strategies. We want to actually see which of these Nash equilibria are sub-game perfect? Let me give myself a bit more room here because I want to keep this in sight. So let's get rid of this and raise this one. So the next question is which of these three Nash equilibria are sub-game perfect? To do that we need to start by identifying the sub-games. And, of course, just having hoisted that board up there, I have to hoist it down again. Okay, so what are the sub-games here? Well the simplest sub-game is this simple sub-game at the end, in which Player 1 moves. That's a very obvious sub-game, is that right? That's a little game within a game, it's a rather trivial game because it's a one player game, but it is a game. </p><p>So let's examine that one first, that's the last sub-game. So the last sub-game here is a somewhat trivial sub-game. It looks like this. Player 1 is the only mover and they're choosing either up or down and the payoffs are (4, 3) and (3, 1): and frankly we don't really care at this point what Player 2's payoffs are because Player 1 is the only person who's playing in this sub-game. But nevertheless, let's put them there. And if we write this up as a matrix--here it is as a matrix. Since Player 1 is the only mover they're choosing between up and down, and the payoffs are (4, 3) and (3, 1), and of course Player 2 doesn't get to move so Player 2 is irrelevant here. And clearly the only Nash equilibria in this game is for Player 1 to choose up. Player 2, it doesn't really matter what they choose, there's nothing they can do about it anyway, but for Player 1 to choose up is the Nash equilibria. So the Nash equilibria in this trivial sub-game is 1 just chooses up. Is that right? </p><p>So let's look at the play induced by our three candidate Nash equilibria in this sub-game. So each of our candidate Nash equilibria--here they are, this one, this one, and this one--have an instruction of how Player 1 should play in this sub-game. And let me just pause a second. The reason that these three equilibria have an instruction for how Player 1 should play in the sub-game is because of our definition of a strategy. Each strategy tells the player how they should move at every information set of that player. So even if the strategy is such that that information set won't be reached, the strategy still has to tell you what you would do when you got there. And now for the first time perhaps we're going to see why that redundancy helps us. So let's look at the instructions. Each of them gives an instruction. The first one tells us to play up in this sub-game. The second one says up again, and notice this was redundant. Once you've chosen Down you know you're not going to get to make a choice at the third node, or your second node, but nevertheless there's the instruction and it says up. The third one says down. </p><p>So this is the instructions of these three equilibria in this little sub-game. This is the play prescribed by these three equilibria in this sub-game. Two of them say up and those ones are going to induce the Nash equilibrium in this sub-game, but the third one does not. The third one says down and that's not allowed. That's not allowed in a sub-game perfect equilibrium because a sub-game perfect equilibrium has to prescribe play in every sub-game that's Nash, and here the third equilibrium is telling Player 1 to choose down which is not a Nash equilibrium in the sub-game. So what are we doing here? Let's make it clear. We're finding the sub-game perfect equilibria. And what we've done is: number 3 is eliminated because it induces play in this sub-game that is not Nash equilibrium, not a Nash equilibrium in the sub-game. </p><p>We're really putting stuff together now: to be able to draw this conclusion, we really used the fact that strategy 3 contained a redundant instruction, an instruction down at this node that was never reached. But that helped us get rid of it. So that one's gone: 3 is gone. Let's proceed. I'm going to run out of board space here. Have people got this one down? I'll bring it back in a second, almost, let me give it a second. What I want to do now is look at the next sub-game. Maybe what I can do if I just remove this comment, I can work on the right hand board, that'll allow you to look at it. So all that comment said was 3 is eliminated. Let me work now on the right hand half of this board, that'll allow it to be up there a bit longer. </p><p>Okay, so let's now look at the next sub-game, and again, we're going to work from the back. So the next sub-game back is the sub-game that starts from this node, the sub-game that starts from that node. So let's identify that in a different color. I used blue so let me use pink. So now we're going to look at this sub-game, this big pink sub-game, and again in this pink sub-game this game looks like this. It starts with Player 2 choosing between left and right, and then Player 1 has to choose up or down, and the payoffs are (1, 2), (4, 3) and (3, 1). Once again let's look at the matrix form of this sub-game, and this is a little bit less trivial than the last one because now there are really two players playing. So here's the matrix that goes along with this. </p><p>Player 1 is choosing between up or down and Player 2 is choosing between left or right. And this is slightly, slightly cheating because in fact, Player 1 of course knows what Player 2 is going to have done by the time she moves, but never mind it'll do for now. Let's just put the payoffs in. So (up, left) is (4, 3) and (down, left) is (3, 1) and (up, right) is (1, 2) and this must also be (1, 2). Everyone happy with that? Just putting the payoffs in. And let's just look again at pure-strategy Nash equilibria here. There are actually mixed ones, but let's just worry about pure ones for now. So the pure Nash equilibria here in this little sub-game are what? </p><p>Well, let's just see. If 2 chose left then 1 wants to choose up. If 2 chooses right it doesn't really matter what 1 chooses because she isn't going to get to move anyway. Conversely, if 1 chooses up then 2 wants to choose left. That's the example of 1 not screwing up, so 2 wants to stay in the game. But if 1 was to choose down then Player 2 would like to get out of the game, so if Player 2 thinks 1 is going to screw up she wants to exit the game. Very quickly we can see there are two equilibria here. One of them is (up, left) and the other one is (down, right). They correspond to playing down this way, that's (down, right) and (up, left) playing up this way. </p><p>Once again, let's look at our three equilibria in the parent game. Here's our three equilibria in the parent game, and let's see what play they induced in this little sub-game. So we'll do exactly what we did before. So 1, 2, and 3: these are our three equilibria from above. And equilibrium number 1, ((Up, up), left) in this game prescribes (up, left). Is that right? Equilibrium number 2, ((Down, up), right) here prescribes up, right. And equilibrium number 3, ((Down, down), right) here prescribes (down, right). So which of these are prescribing Nash equilibria in the sub-game? </p><p>Well (up, left) is an equilibrium. It's that one. (up, left) is an equilibrium so this is okay. And (down, right) was an equilibrium, so 3 is okay in this sub-game. But (up, right) is not a Nash equilibrium. So in this sub-game, Nash equilibrium number 2, the ((Down, up), right) equilibrium is prescribing play that is not a Nash equilibrium in the sub-game: so it's eliminated. It can't be a sub-game perfect equilibrium. So here 2 is eliminated since it induces non-Nash equilibrium play in this sub-game. Now we're done, we know about the whole thing. </p><p>So what we did here we started with the whole game, we found there were three Nash equilibria, we found that only one of them agreed with backward induction. We then looked at the sub-games. We first of all looked at that blue sub-game and we found that one of the equilibria, equilibrium number 3, was eliminated. Equilibrium number 3 is not prescribing Nash behavior in this sub-game. Then we looked at the slightly more complicated sub-game, the pink sub-game, and we found that equilibrium number 2 prescribes the behavior (up, right) which is not Nash in this sub-game. At this stage we've eliminated two of the three equilibria and we're just left with one. And the one we're left with, the only sub-game perfect equilibrium, the only equilibrium that wasn't eliminated by the fact that it would prescribe bad behavior in sub-games, the only SPE is number 1: which is (Up, up) and left. </p><p>What do we notice? We notice that that's the equilibrium, that's the play that backward induction would have selected. So notice this is the backward induction prediction. So what are the lessons here? The lessons here are that our new idea, the idea of sub-game perfect equilibrium is pretty easy to go about finding. You just look at sub-games and check that the play in each sub-game has to be Nash play. If you start at the back, you construct it by rolling backwards, much like we did backward induction. Start at the last sub-game and work backwards. The second thing is--not surprisingly given that remark--not surprisingly, where backward induction applies, for example in this game, the sub-game perfect equilibrium will find the equilibrium that is consistent with backward induction. </p><p>Remember that was our aim last week. We wanted a way of refining Nash equilibrium to throw away those Nash equilibria that were inconsistent with backward induction. So sub-game perfect equilibrium has done that. It tells us now if backward induction applies, the Nash equilibria you should focus on are the sub-game perfect equilibria. Indeed, most people in the class played that equilibrium just now. Okay so that was really what I wanted to say about this example, but let me just make a remark in passing. I made this remark in the middle, so let me just make it again. </p><p>When we write down strategies, those strategies tell us what seem to redundant moves. But being forced to write down those redundant moves is useful because it allows us to model what other people think you would have done at those later nodes. And sometimes I have to think what you think I would have done at this later node before I decide not to go down that branch of the tree. So being able to write down everything in a strategy allows us to have everything in front of us and makes that analysis simple; and that's exactly what we did here. </p><p>So this was a fairly mundane example, because in particular we didn't use any kind of information set. So next let's look at an example that does use some information sets. So: new example. Let me clean this off. Once again, I want to play this example. But what I'd really like to do--we're going to call this game the matchmaker game--and what I'd actually like to find out is: do we still have our couple we tried to send on a date? Our hapless couple we tried to send on a date at about week three. Are you guys still here? There's the guy, what's your name again? </p><p><b>Student:</b> David.</p><p><b>Professor Ben Polak: </b>David, and what was the--is she hiding? There she is thank you. Your name was?</p><p><b>Student:</b> Nina.</p><p><b>Professor Ben Polak: </b>Nina and David. Good, can we get some mikes to Nina and David actually? Let me do it. I'll go on talking while I'm doing this&ndash;. Where's David, and Ale can you get a mike to Nina? That would be great thank you. All right, so for weeks we've been trying to get this couple to go on a date. It's our attempt to get economics majors to become real people. It's a hard thing to do and they're kind of the hapless couple because first we sent them to the movies and they end up going to different movies; and then we sent them off for a romantic weekend in New England and they end up doing different things, one went to the theatre and another one apple picking, I forget which way around it was. And at this point, I figure I'm a pretty bad matchmaker so what I'm going to do is. I'm going to introduce a third player into the game as the matchmaker. </p><p>So first of all I'll write down what the game is. So the game at this point--the game is going to look like this. Player 1 is the matchmaker, we can call him Player M if you like, and he has a choice, he or she has a choice. She could not send the couple out on a date or she could send the couple out on a date. But being a better matchmaker than me, if she sends them out on a date, she's going to stake some money, she'll pay for the date. And in the date once again they're trying to meet, and once again, unfortunately, they haven't figured out where to meet. We'll put the payoffs in, in a second and we'll tell you what the strategies are. </p><p>So what I'm going to assume here is I'm going to let Jake our T.A. be our matchmaker. And the reason I'm choosing Jake is I think he's the nearest thing I have in mind in this class to being a Jewish mother. I mean: he's neither Jewish--I think he's not Jewish and he's not my mother, but he is the T.A., who's responsible for bringing some drink everyday in case I pass out in the lecture. So that's the nearest thing I can think of. So Jake's going to be our Jewish mother, and Jake's going to either send these guys on a date or not. And Jake's smarter than me at this: he's actually good at matchmaking. And what he's going to do is, he's going to send them somewhere where they really--he knows Yale students better than I do--and he's going to send them really somewhere where they're going to meet. </p>
<p>So he's going to send them to go to the same lecture class next year, and then they'll be sitting in the aisles in this huge lecture class and they're bound to meet. All of you have sat next to other people at some point. So that seems like a good idea. So the classes he thinks of sending them to--he says go to a large lecture class. So they're either going to go to the Gaddis class which is called "Cold War"; or to the Spence class which is called "China." Everyone know a little bit about these classes? These seem like reasonable classes to go to meet your--to have a date--to meet somebody. I mean the Cold War can be a fun class, I mean, you hope it isn't a prediction of the future relationship but the Cold War seems all right. And China is, by all accounts, a fantastic class. It involves, something involving 20 million people, most of them were in the class together, so it's a pretty big class. </p><p>So let's do that. Unfortunately Jake makes the same mistake I do, he's not going to tell them which class to go to. So they have to decide whether to take Gaddis or Spence, and once again they're coordinating. We'll call them Players 2 and 3. So here they are trying to coordinate, and the payoffs are as follows. So let's put in Jake's payoffs first of all. So if they manage to coordinate, first of all, if Jake doesn't send them, everybody gets nothing. And if Jake does send them and they coordinate Jake makes 1 because he feels really happy about this. After all there must be some motivation for people matchmaking. So if they coordinate down here Jake gets 1 as well, but if they fail to coordinate, Jake feels rotten about it, particularly because he paid for them to go this class--whatever the cost of a class at Yale is--which is probably quite a lot actually. So okay we'll call it 1 though and otherwise the payoffs are exactly the same as the payoffs we used when we looked at this game earlier on in the course. So the payoffs are going to be (2, 1) here and (0, 0) if they fail to coordinate; and (0, 0) here if they fail to coordinate; and (1, 2), (1, 2) here. </p><p>So the implication of this is that Player 2 who we'll assume is David, so David would like to meet Nina but all other things being equal, he'd like to meet her at the Cold War. And Nina would like to meet David, but all things being equal, she would like to meet in China. Not literally in China, but in the class. So this is our game and we're going to analyze this game, but before we analyze it let's try and play it. So what we need to do is first of all let's make sure things work smoothly, let's have David write down which class he's going to choose, and Nina write down which class she's going to choose. I've lost sight of Nina. Somebody has to point out--there she is. Write down which class you are going to choose. Something been written down? Jake you got your mikethere, so Jake are you going to send this hapless couple or not? </p><p><b>Student:</b> So I have Dave in my section actually and I hear how much he's been talking about Nina, so I'm going to roll the dice and send them.</p><p><b>Professor Ben Polak: </b>He's going to send them good, so we have them going off to this class and now let's see what they wrote down. So Dave what did you write down?</p><p><b>Student:</b> I'm going to give in and go to China.</p><p><b>Professor Ben Polak: </b>You're going to go to China and Nina?</p><p><b>Student:</b> I chose S.</p><p><b>Professor Ben Polak: </b>Great, so they managed to meet, so it's a successful date. So let's give them a round of applause. I hear it's a great class too. And in fact, I don't think it's going to happen forever because I think he must be approaching retirement. So that seems a pretty good choice. So good: that worked very well.</p><p>Let's have a look now at this. Let's analyze this game and see what we can do with this game. So how are we going to analyze this game? So no surprise we're going to use the idea of a sub-game perfect equilibrium, I'll collect the mikes later don't worry. So we're going to use the idea of a sub-game perfect equilibrium. So how do we figure out how to work out what a sub-game perfect equilibrium is? We're going to use the same basic idea that we used--what we've been using all along in backward induction. It's the same idea in the game we just looked at just now. What we're going to do is, rather than start from the last decision node (we can't do that anymore) and work backwards, instead of doing that, we're going to start from the last sub-game and work backwards. In this example it's pretty obvious what the last sub-game is. The last sub-game, the game within a game--there is only really one--the game within the game is this object here, is that right? This is the game within the game. </p><p>Now, I could at this stage, I could do something else. I could write down the whole matrix for the whole game and have Jake choose the matrix, and Dave and Nina choose the row and the column, but that's going to get us astray. So I mean we could do that but let's not worry about that. Let's just start doing things backwards. So when we do things backwards we'll start at the last sub-game and that last sub-game is an old friend of ours, it looks exactly like this. Let's just write it in. </p><p>So it involves Players 2 and 3. And 2 was choosing between Gaddis and Spence. And 3 was choosing between Gaddis and Spence. And their payoffs were--let me leave a space here--so it was (2, 1), (1, 2), (0, 0), (0, 0). So here are the payoffs of the relevant players in the game but while we're here why don't we put in Jake's payoffs as well. So Jake's payoffs were 1 here, 1 here, -1 here, and -1 there. So the only relevant players here are Players 2 and 3, but I've put Player 1's payoffs in as well because: why not, why not just keep track of it. Everyone happy that that exactly describes this little game? </p><p>For all intents and purposes we can forget the first payoff but there it is. This is a game we've seen many times so far. It's the battle of the sexes, or the battle of Dave and Nina, and in this game we already know what the equilibria are. So the equilibria here, let me just underline the best responses. So if Nina is choosing Gaddis then Dave chooses Gaddis. If Nina is choosing Spence then Dave would like to choose Spence and conversely. So I've just underlined the best responses for the players who are actually involved in the game and haven't bothered underlining anything for Jake because he isn't a player in this game. Does that make sense? So the pure Nash equilibria in this game are essentially (Gaddis, Gaddis) or (Spence, Spence). That's pretty easy. </p><p>From Jake's point of view, each of these pure strategy Nash equilibria yield a payoff for him of what? What does he get? If they go to (Gaddis, Gaddis), he's happy that they met and he gets 1 and if they choose (Spence, Spence), he's happy that they met and he gets 1. Jake himself doesn't really mind whether Dave and Nina learn about China or learn about the Cold War. He just wants them to meet. So, both of these yield 1 for Jake. They both yield a value of 1 for Player 1, who is Jake. So from Jake's point of view, going back a stage--what we're going to do now, just as we did with backward induction, we're going to roll the game back. So we started by analyzing this sub-game and now we're going to roll it back a stage, just as we did with backward induction. </p><p>So when we roll back Jake is moving here, if Jake chooses not to send them then we get (0, 0, 0) but the key part of this is the first 0, that's Jake's payoff. And if Jake sends them, then what Jake gets is the value to Jake--I'll put in Jake but value to Player 1--okay value to Jake of the Nash equilibria in this sub-game. Not a big thing to write, but that's what Jake gets. And the others do too: the others get that as well. In this case, rather than writing that long piece, this is just equal to 1. Is that right? So if Jake sends them he knows that they're going to play a Nash equilibrium in this sub-game, or he believes they're going to play a Nash equilibrium in this sub-game. And either of those two Nash equilibria in the sub-game yield a payoff to Jake of 1. </p><p>So actually, one of them is 1, one of them yields payoffs of (1,1,2); and the other yields payoffs of (1,2,1). But since Jake is the only mover here, let's just focus on Jake. So from Jake's point of view he's really choosing between 0 and 1, so he's going to choose send. So the sub-game perfect equilibrium therefore--there are actually two of them here--one is (send, Spence, Spence), that's what actually happened. But there's another one which is (send, Gaddis, Gaddis) that would also have been a pure strategy sub-game perfect equilibrium. So in either case, what we did, just to remind ourselves, we first of all solved the equilibrium down the sub-game, the equilibrium in this blue sub-game. We figured how much that equilibrium was worth for everybody--but in particular for Jake, but for everybody--and then we rolled that payoff back and looked at Jake's choice. </p><p>In this particular case that game has two equilibria, (send, Spence, Spence) and (send, Gaddis, Gaddis). However, some of you must be suspecting at this point that there's actually another sub-game perfect equilibrium here. How do we know that? Well let's just think about this game. We've been trying to send this couple on a date all semester. They haven't gone on a date all semester. I'm embarrassing them, but they haven't gone on a date all semester. So there must be some possibility that they would fail to coordinate. It would be a pretty weird notion of equilibrium that concluded that they always manage to coordinate and hence Jake always wants to send them. Is that right? </p><p>So let's also look at the other equilibria here. Now the reason there's another equilibrium in this sub-game--sorry, the reason there's another equilibrium in the whole game--the reason there's another sub-game perfect equilibrium in the whole game is that there's another Nash equilibrium in the sub-game. What's the other Nash equilibrium in the sub-game? They could mix. So it turns out that in the sub-game (here it is) there's also a third mixed equilibrium. There is a mixed Nash equilibrium. Now we know how to work that out. We could write down a P and a Q, and we could look for those indifference conditions and solve it out. But this is a sub-game, sorry this is a game, this sub-game corresponds to a game we've seen many times in this class so far, and I think we probably remember what that equilibrium is. Is that right? I do anyway, so let's see if you remember it as well. I'll write it down and we'll see if you all looked alarmed. </p><p>So I claim the equilibrium, the other equilibrium has Dave playing with probability (2/3, 1/3), and has Nina playing with probability (1/3, 2/3). So this is another equilibrium in the sub-game. People remember that this was an equilibrium in battle of the sexes? Yeah, people are nodding at me, yeah okay. So it isn't too unintuitive. We all know how we'd work it out. We could go back and put in the P and the Q, but it isn't too unintuitive, it has Dave going more often to the lecture course that he would prefer all other things being equal; and it has Nina going more often to the lecture course that she would prefer all other things being equal. And they do so in just such a way as to make each of them indifferent. </p>
<p>Now, this sub-game induces a different value for Jake. So suppose Jake thinks: "I trust Dave and Nina to play a Nash equilibrium in their sub-game but I don't know which one it is and I think maybe they're going to play this one." So suppose Jake thinks that this is the equilibrium that Dave and Nina are going to play. So now should Jake send them or not? Well let's work it out. So now if he sends them, if Jake sends Dave and Nina, or more anonymously if Player 1 sends Players 2 and 3, then with what probability will they meet? Well this is just a little math exercise, let's have a look at the game again. </p><p>So Dave is playing 2/3, 1/3, is that right? Nina is playing 2/3, 1/3, is that right? So the probability of their meeting is the probability of this box, they could meet at Gaddis, plus the probability of this box, they could meet at Spence, is that right? So the probability of this box is 1/3 x 2/3 so this box has probability 2/9 and the probability of this box is 2/3 x 1/3 so this box has probability 2/9. So the probability of their meeting is 2/9 + 2/9 that makes 4/9. Everyone okay with that? So if Jake sends Dave and Nina and they play this mixed strategy equilibrium, then they meet with probability 2/9 + 2/9, 2/9 at Gaddis, 2/9 at Spence for a total of 4/9, which means they failed to meet with probability--well if they're meeting with probability of 4/9 what must be the probability that they're failing to meet? 5/9, thank you. So they fail to meet with probability of 5/9. </p>
<p>So Jake's expected payoff if he sends them, the value for Jake of this equilibrium is what? So the value to Jake of this Nash equilibrium, if he sends them, is 4/9 x 1 + 5/9 x -1 for a total of -1/9. Everyone okay with that? So if Jake sends them they fail to meet 5/9 of the time and he gets -1 each of those times. They succeed in meeting 4/9 of the time, he gets +1 each of those times, so his expected payoff, his expected value from sending Dave and Nina on the date is -1/9. So from Jake's point of view, what this game looks like, if he thinks that this is the Nash equilibrium being played: if he doesn't send he gets 0 and if he does, he gets the value of this Nash equilibrium, which in this case is -1/9. So he's not going to send and the SPE here is (not send, "mix," "mix") where this is the mix. </p><p>So there's a third equilibrium here in which our matchmaker says this hapless couple is just too hapless: they're going to play the mixed strategy equilibrium in which case it isn't worth my while sending them on the date. You guys were lucky because Jake chose the other equilibrium. He figured you were playing the other equilibrium, which it turned out that you were. So in this game there were three sub-game perfect equilibria, one for each of the Nash equilibria in the sub-game, as it turned out. There was one in which Jake sent them and they coordinated on the pure strategy equilibrium in the game (S,S). There was one in which Jake sent them and they coordinated on the pure strategy equilibrium in the sub-game (G,G). And there's one in which Jake didn't send them, but had he had in fact sent them, they would have both mixed, and hence, for a lot of the time, failed to coordinate. </p><p>Now, what's the big lesson here? The big lesson of the first game we saw this morning was that sub-game perfect equilibrium implies backward induction. The big lesson of this game--other than the fact that we're getting closer to getting Dave and Nina on their date--the big lesson of this game is to show that to find sub-game perfect equilibria, all you have to do is keep your head and solve out the Nash equilibria in each of the sub-games, roll the payoffs back up, and then look for behavior up the tree. Once again, you look for the Nash equilibria in each of these sub-games, roll the payoffs back up, and then see what the optimal moves are higher up the tree. </p><p>So we have time to do one more example, and the third example I want to do is more of an application. So far we've seen some fairly simple examples. Now I want to do an application. The application I want to do is kind of a classic business school case if you like, or a mini case involving strategic investment. The game is this, or the setting is this. There are two firms, we'll call them A and B. And these two firms, initially, before we start considering what we're actually going to talk about, initially they are playing Cournot competition. So two firms and they're playing Cournot competition. And we can imagine that they're producing fertilizer. </p><p>And let's be specific here, let's assume that the prices in this market are given by the following demand curve 2 - 1/3 x [qA + qB], so this is the demand curve that they face. We'll assume that costs, marginal costs, c is equal to $1 a ton. So this is the price in dollars per ton, and the costs are $1 per ton. In a minute what we're going to do is we're going to consider a change in this game, but before we do that let's just remind ourselves what the Cournot equilibrium of this game would look like. Let's do a bit of a review. So it's been a while since we've seen Cournot, so let's remind ourselves. </p><p>So I claim that the quantity, the Cournot quantity chosen Q* has the formula [a--c]/ 3b. Is that right? If you go back in your notes you'll find it. I'm not going to re-solve it here. We've done it many times. So [a--c]/3b trust me, is what came out of our calculation before the mid-term. What I want to do is, I just want to make sure we can translate that into numbers here. Sorry for having it in letters, but let's translate it into numbers. So in particular, this a is this 2, is that right? This c is this 1 and this b is this 1/3, is that right? So let's just put that down. So in this case this is [2--1]/[3 x 1/3], so this says that this is a million tons. So the quantity here, the Cournot quantity is a million tons each. So: one each. So in this equilibrium, each of these two firms is producing a million tons of fertilizer. </p><p>What else do we know? We know therefore what prices must be. Let's just do that before we even get started. So prices must be [2 - 1/3] times the quantity that the first firm produces plus the quantity that the second firm produces. So that's 2 - 2/3 so that should be 4/3, if I've got that right, or one and a third. So prices here are $1.33 per ton. Finally profits. So profit for each firm here, in this equilibrium, before we even start the game, or start the more interesting part of the game, profit is what? So they're going to get $1 and 1/3 for every ton they produce. It's going to cost them $1 to produce each ton and they're producing one million of these things. So their profits are 1/3, if these are millions, their profits are 1/3 of $1 million dollars. So this is their per period profit, in each period they're doing this, each year they're doing this and this is their profits in each period. </p><p>So this is a simple model that we've done many times before. This is Cournot, and now we're going to make it more interesting. If the algebra here was a bit quick don't worry about it, check it at home, it's just basic, basic algebra. So now suppose that you are the manager of Firm A. So it's a classic business school case. I'm looking at my business-school students in the balcony. You're the manager of Firm A and you have to choose whether to accept an offer to rent a new machine. So this new machine has two features. </p><p>The first--well three features--the first feature is it only works for A. So this machine is being offered to you. It wouldn't fit in to Firm B's technology, so this is only being offered for A. The second feature of this machine is it costs $0.7 million dollars in rental. So each year you rent this machine, you'd have to pay $0.7 million dollars. But that's the bad news. The good news is it will lower A's costs to $0.50 a ton. So classic business-school situation. You're the manager of a firm. You're involved in competition with another firm, B. And suddenly an opportunity comes along to rent some new technology. It's going to cost you $0.7 a year to rent this machine, but it will lower your costs by $.50 a ton. </p><p>So this is a classic thing that you might be asked in your interview for Morgan Stanley next week. How many of you are interviewing with investment banks? No one's going to admit it. In a couple of years, when you're interviewing with these guys. So what's the obvious question? The obvious question is, should you go ahead and rent this new technology or not? Should you rent it or not? To rent or not to rent? Less dramatic than it's equivalent question in the English class, but important nevertheless. This board is stuck unfortunately, so I'll have to write there a bit more. </p><p>So what I want to do is I want to analyze this three times and each time I analyze it, I want us to see what I'm doing--what mistakes I'm making--because I want you guys, when you interview with Morgan Stanley about this kind of thing, to impress them so that they tell lots of people to come to Yale and preferably give lots of money to Yale. So we're going to look at this way three different times. And the first thing we're going to do, the first way we're going to look at this is look at it as if we were accountants. We're going to look at the accountants' answer to this question, and some of you may decide you don't want to interview with Morgan Stanley or McKinsey, you might want to interview with some accounting firm when you leave Yale. God forbid, but you might. </p><p>So let's have a look at how the accountants would answer this question. So I think the accountants would do this. They would say--but before we do this let's have a poll. How many people think you should rent? You've had some time to think about it now. So how many people think you should rent the new machine? How many people think you should not rent the new machine? You're not allowed abstensions here. Let's try it again: no abstentions right. You can't abstain in an interview. So you're on the spot, you're in the boardroom, how many think you should rent the new machine? Raise your hands. Wave them in the air. How many people think you should not rent the new machine? So we're split kind of down the middle, I'm looking at my MBA students to see which they voted. Which did you guys vote, rent or not rent? Rent, the MBA's seem to think rent. We'll see if that's right. </p><p>So let's move forward, so accountant's answer. So I think what the accountant's going to say is this. They're going to say, right now you're producing a million tons a year. The new machine saves you--so, let's put per annum--let's try and be fancy--so a million tons per annum. The new machine saves you $.50 per ton. So if you rent this new machine, you're producing a million tons a year, it's going to save you $.50 a ton. So it's going to save you half of one million a year in variable cost. Those people who were in 115 or 150 will know what I mean by variable cost. It's the cost you're going to save in the actual production of your fertilizer. So it saves you half a million a year. Unfortunately, it costs you the cost of the machine, which is a fixed cost of $0.7 million a year. And .7 is bigger than .5, so you should not rent. So .5 is less than .7 so don't rent the machine. </p><p>How many of you said no? That's kind of the back of the envelope calculation you were doing, is that right? Kind of the back of the envelope calculation that accountants do. So what's going on here? Our business school student up in the balcony says you should rent. He took accounting. I know he did that because you have to take accounting at business school. So what's wrong? Did he fail accounting, or is this answer wrong? The answer is wrong. There's two things you need to know about accountants. One is that they're usually boring, and the other is that they're often wrong. They're more often boring than wrong, but they're almost always boring. So this answer is kind of boring, and it happens also to be wrong. </p><p>Why is it wrong? It's wrong because we made an assumption here that's not a good assumption. We made the assumption that you're going to go on producing the same amount per year after you've invested in the new machine that lowers marginal costs as you are producing beforehand. We made the assumption that it would lower--we know it lowers your costs--and we assumed implicitly that you would go on producing a million tons a year, but that's not right. So let's try and have a more sophisticated answer, and if you want to be more sophisticated and less boring than accounting, what class would you want to take? Economics probably right, so let's have a look at an Economics answer. </p><p>Let's look at an Economics 115 answer. How many of you have taken Economics 115? How many of you are in 115 at the moment? Quite a few, okay so let's have a look at the Economics answer. Let's see why that previous answer was wrong. So here is qA and here is cost of a $1, your new cost will be $.50. So I'm putting prices and costs on this axis. And here is your residual demand curve. This is the demand curve you face after the other guy has finished producing, so this is your residual demand curve. It's the demand curve on that part of the market you're supplying, or not being supplied by the other side of the market. And to figure out your optimal quantity on your residual demand curve what you should do. It's like you're a monopolist on this residual demand curve, so you should set what? If the answer isn't backward induction it's probably marginal revenue equals marginal cost right? </p><p>So let's try that. So here's the marginal revenue curve roughly speaking, should be twice as steep. This is residual marginal revenue and here's what you used to produce. So we know what this is: this was a million tons. This was marginal revenue hitting marginal cost. Now your costs have gone down so notice that your quantity, your new quantity has gone up. Your new quantity has gone up because you slid down the marginal revenue curve as the marginal cost curve dropped. Is that clear to everybody? So this is the kind of picture that you probably saw a lot of in 115, is that right? Or in 150 for that matter, is that correct? </p><p>So notice in this picture we can actually see the accountant's answer, the boring answer. The boring answer is this rectangle. This rectangle, this is the accounting answer. This rectangle is 0.5 x 1, so it comes out as a 1/2 and that was the accountant's answer. And what did they miss. What did the accountant's miss? They missed the triangle. So I told you they were boring, they were a little bit square, so they tend to miss triangles. So here's the triangle that they missed. So we missed this triangle. And how big is this triangle? Well we could do it at home, it's 1/2 base x height. So we could figure this out. We know the slope of this line. We know the slope of this line is 1/3. We know the slope of this line is 2/3. We know that the height of this triangle is 1/2. We could figure out what the width is as well, therefore we could do half base times height. Turns out that this has area--I did this at home so let me just write it down--it has area 3/16. </p><p>So again, everyone could figure out the area of a triangle at home, is that right? You all know that from your probably junior high school geometry. So assuming I did it correctly at home, this is 3/16 which is approximately 0.19. So we missed this 0.19. So how are we doing now? So we know from the accounting answer we had a 1/2 in savings. We know from the Economics answer that we should add another 0.19 to this--that's the triangle--for a total of 0.69 but unfortunately this is still less than .7 which is the cost of the machine, the per annum cost of the machine. So it looks like we should still not rent. </p><p>So even after taking Economics 115, which is a good thing to do--it is much less boring than accounting and will get you the accounting answer anyway if you do things carefully--we still end up concluding you shouldn't rent. But our guy from the business school said you should rent, right? So did he fail Economics as well as accounting, or is this answer wrong? This answer is still wrong. </p><p>[I didn't really want to delete that. That's a shame. Oh well, I've done it now. People've got those numbers somewhere. I hope they are in my notes.] </p><p>This answer is still wrong. We need to get the right answer. So what we need is a third answer which is the Game Theory answer, which is also known as the right answer. What are we missing? What's wrong with the Economics answer? Somebody? Everyone knew it was wrong, why is it wrong? It looked pretty good, what's wrong with it? Let's bounce down here. Somebody in the front row will help me. What did I do wrong?</p><p><b>Student:</b> Firm B also changes its quantity.</p><p><b>Professor Ben Polak: </b>Right, in the accounting answer, we assumed that Firm A kept its quantity fixed, that we kept our quantity fixed, and that was wrong. But in addition, Firm B is going to change its quantity, isn't it? Firm B is going to change its quantity. Let's remind ourselves why. We're still playing Cournot competition, here's our Cournot diagram with qA and qB. So prior to making this investment the model is symmetric. Here it is. And this is the old best response of Firm A, and this is the best response of Firm B. What we learned just now in the Economics answer is what? We learned that Firm A, as its costs go down, will produce more for each possible quantity that Firm B produces. So regardless of what generated this residual demand curve as the costs go down for Firm A, it increases its quantity. So we know that. </p><p>So what's that telling us? It's telling us that the new best response of Firm A is shifted to the right. This is the new best response for Firm A. It's shifted to the right. It now produces more for any given quantity that Firm B produces. So qA has gone up. That was our Economics answer. But that leads Firm&nbsp;B to do what? To produce less. That leads to qB producing less. Notice we slid down Firm B's best response line from the old equilibrium to the new equilibrium, and at the new equilibrium Firm B's production has gone down. By the way, what kind of game is it where as Firm A increases its strategy, Firm B decreases its strategy in response? <i>Strategic substitutes</i>, right. </p><p>So because this is a game of strategic substitutes--good interview word, good word to mention in an interview--because this is a game of strategic substitutes, we know that Firm B reduces its quantity. As Firm B reduces its quantity is that good for A or bad for A? It's good for A, right? This is good for Firm A, it softens competition. As a consequence, it leads to an increase in profit. Again--we don't have time today--but what we could do is we could go back and we could recalculate the new Cournot equilibrium. We could calculate this--we could as a homework exercise try it--calculate the new Nash equilibrium and notice this is a Nash equilibrium in a sub-game. </p><p>Why is this a sub-game? Because Firm A made its decision whether to buy this new machine or not, and then they played Cournot. So the Cournot game, the game up here, what is this? This is the sub-game. This is the diagram of the sub-game, if you like. It's the best responses of the sub-game. So what sub-game perfection tells us to do here is first of all, work out the new equilibrium in the sub-game, work out how much that new equilibrium is worth for Firm A and then roll it back to the investment decision. It turns out when we do that we get an extra $.31 million. So we could do it at home, we get an extra $.31 million. So it turns out that our MBA student was right, good. It turns out that if you add this .31 to the .69 we had already we get 1, which of course is much bigger than .7 and indeed you should rent the machine. </p><p>Now, I want you to have two take away lessons from this game. The first take away lesson is this. When you're analyzing a game like this, be it in the real world or in a job interview, the first thing you want to do is what? You want to look at the sub-game. You want to look at what would happen if you did invest and solve out the new Nash equilibrium in that sub-game. Then you want to roll back the value of that sub-game back into the initial decision which is the strategic investment decision whether to rent this machine or not. So schematically, the game looks like this: rent or not rent, and in either case you play Cournot. There's a sub-game in each case. In this case you play symmetric Cournot, when you both have the same costs; and here you play asymmetric Cournot, where you have different costs. And the way we analyze this game is, we solve out the symmetric Cournot, we actually did that up front. We now solve out the new equilibrium in this asymmetric Cournot game, this one here. This is the old one and this is the new one. Solve it out. Work out how much profit you're going to get. And roll that back remembering that it costs you $.7 million to make this step. So that's the first take away lesson. </p><p>But the second take away lesson is more general, so let me just pause to get everyone to wake up again so I make it. The second take away lesson is this., What tips the balance here from the Economics answer and the accounting answer, were the strategic effects. It was the strategic effect. This is a strategic effect. It was the effect of the other firm or other players changing their behavior. And the most common mistake to make when you're thinking about strategic decisions is what? It's to forget that they're strategic. It's to forget that the other players are going to change their behavior. In this example, the other firm cuts back its production so much as to make that investment profitable. </p><p>But let me give you two other examples. Example number one. You're designing a tax policy for the U.S.. The dumb way to analyze this is to say look at what people are doing now, push through the new tax numbers, and act like an accountant and crunch out how much money the government's going to make. Why is that wrong? Because you're forgetting that as you change the tax code people's behavior changes. Incentives change and people's behavior changes. It leads to a mistake in designing the tax code. You need to take into account strategic effects: how behavior changes. </p><p>Example number two, closer to home. You're designing a new curriculum for Yale. So you change the rules of the curriculum and when analyzing it you say--I wouldn't say this but some people on the committee might say this--under these new rules, if we look at what people used to do, they will now do more of this and less of that, and they'll learn this and learn that. What are you missing? You're missing that students are players and students change their behavior as you change the curriculum rules. So the biggest lesson of today's class is don't be like an accountant, partly because it's boring and you won't go on your date, and partly because you'll miss out on these important strategic effects. We'll come back and look at more on Wednesday. </p>
<p></p><p>[end of transcript]</p><p></h2>
  </p>
</div>
<p><a id="backToTop" href="#top">back to top</p></a>